import requests
import csv
import re
import time
import random
from bs4 import BeautifulSoup
from urllib.parse import quote

# --- é…ç½® ---
INPUT_FILE = "urls.txt"
OUTPUT_FILE = "huawei_corpus_google.csv"
KEYWORD = "Huawei"  # å¦‚æœä½ ç¿»è¯‘æˆäº†ä¸­æ–‡ï¼Œè®°å¾—æŠŠå…³é”®è¯ä¹Ÿæ”¹æˆ "åä¸º" æˆ–ä¿æŒè‹±æ–‡åŒ¹é…

# ä½¿ç”¨ Google ç¿»è¯‘ä½œä¸ºä¸­è½¬çš„å‡½æ•°
def get_via_google_translate(original_url):
    # å¯¹åŸå§‹ URL è¿›è¡Œç¼–ç ï¼Œé˜²æ­¢ç‰¹æ®Šå­—ç¬¦ç ´å Google é“¾æ¥
    encoded_url = quote(original_url, safe='')
    # æ„é€  Google ç¿»è¯‘ä¸­è½¬é“¾æ¥ï¼ˆè¿™é‡Œç¿»è¯‘æˆä¸­æ–‡ zh-CNï¼Œæ–¹ä¾¿ä½ æŸ¥çœ‹ï¼‰
    translate_url = f"https://translate.google.com/translate?sl=auto&tl=zh-CN&u={encoded_url}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept-Language': 'zh-CN,zh;q=0.9',
    }
    
    try:
        # å¢åŠ ä¸€ç‚¹ç‚¹å»¶è¿Ÿï¼Œè™½ç„¶ Google ä¸å¤ªä¼šå°ä½ ï¼Œä½†æˆ‘ä»¬è¦ä½è°ƒ
        time.sleep(random.uniform(1, 2))
        
        response = requests.get(translate_url, headers=headers, timeout=20)
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸæ‹¿åˆ°äº† Google çš„å“åº”
        if response.status_code != 200:
            print(f"ğŸ›‘ Google ç¿»è¯‘ä¸­è½¬å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return []

        soup = BeautifulSoup(response.text, 'html.parser')
        
        # åœ¨ Google ç¿»è¯‘çš„é¡µé¢ä¸­ï¼ŒåŸç½‘é¡µçš„å†…å®¹é€šå¸¸ä¼šè¢«æ”¾åœ¨ç‰¹å®šçš„æ ‡ç­¾é‡Œ
        # æˆ–è€…ç›´æ¥æŠ“å–æ‰€æœ‰çš„æ®µè½ã€‚å› ä¸º Google ä¼šä¿ç•™åŸæœ‰çš„ <p> æ ‡ç­¾
        paragraphs = soup.find_all(['p', 'div', 'span'])
        
        full_text = ""
        for p in paragraphs:
            # è¿‡æ»¤æ‰è„šæœ¬å’Œæ ·å¼ä»£ç 
            if p.parent.name not in ['script', 'style']:
                full_text += p.get_text(" ", strip=True) + " "

        # è¯Šæ–­æ‰“å°
        print(f"ğŸ“¡ ä¸­è½¬æˆåŠŸ | é¡µé¢æ–‡æœ¬é•¿åº¦: {len(full_text)}")

        # åˆ†å¥åŒ¹é…ï¼ˆåŒ¹é… Huawei æˆ– åä¸ºï¼‰
        sentences = re.split(r'(?<=[ã€‚ï¼Ÿï¼.!?])\s*', full_text)
        
        # åŒ¹é…è‹±æ–‡ "Huawei" æˆ– ä¸­æ–‡ "åä¸º"
        matches = [s.strip() for s in sentences if ("Huawei" in s or "åä¸º" in s) and len(s.strip()) > 10]
        
        return matches

    except Exception as e:
        print(f"âŒ Google ä¸­è½¬å¼‚å¸¸: {e}")
        return []

def main():
    print(f"ğŸš€ å¯åŠ¨æ–¹æ¡ˆä¸‰ï¼šGoogle ç¿»è¯‘ä¸­è½¬æ¨¡å¼...")
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8-sig', newline='') as f_out:
        writer = csv.writer(f_out)
        writer.writerow(['åºå·', 'åŸé“¾æ¥', 'æ ‡é¢˜', 'åŒ¹é…è¯­æ–™'])

        with open(INPUT_FILE, 'r', encoding='utf-8') as f_in:
            lines = [line.strip() for line in f_in.readlines() if ',' in line]

        count = 1
        for i, line in enumerate(lines):
            title, url = line.split(',', 1)
            # ä¿®å¤ URL
            if "https://" in url[8:]: url = "https://" + url.split("https://")[-1]

            print(f"[{i+1}/{len(lines)}] æ­£åœ¨é€šè¿‡ Google è®¿é—®: {title[:20]}...")
            
            sentences = get_via_google_translate(url)
            
            if sentences:
                for s in sentences:
                    writer.writerow([count, url, title, s])
                    count += 1
                print(f"âœ… æˆåŠŸæå– {len(sentences)} æ¡")
            else:
                print("â“ æœªå‘ç°å…³é”®è¯")
                
            # æ¯ 10 ç¯‡ä¿å­˜ä¸€æ¬¡
            if (i+1) % 10 == 0:
                f_out.flush()

    print(f"âœ¨ ä»»åŠ¡ç»“æŸã€‚")

if __name__ == "__main__":
    main()