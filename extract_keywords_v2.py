#!/usr/bin/env python
# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
import csv
import re
from urllib.parse import urljoin, urlsplit, urlunsplit, parse_qsl, urlencode
import time
import random

# --- è¾…åŠ©å‡½æ•°ï¼šå¤„ç† URL å‚æ•° ---
def _set_query_param(url, key, value):
    parts = urlsplit(url)
    query = dict(parse_qsl(parts.query, keep_blank_values=True))
    query[str(key)] = str(value)
    new_query = urlencode(query, doseq=True)
    return urlunsplit((parts.scheme, parts.netloc, parts.path, new_query, parts.fragment))

# --- æ ¸å¿ƒä¿®æ”¹ï¼šç²¾å‡†æå–æ–‡ç« é“¾æ¥ ---
def _collect_links_from_html(soup, base_url):
    """
    åªæå–çœŸæ­£çš„æ–‡ç« é“¾æ¥ï¼Œæ’é™¤åˆ†é¡µæŒ‰é’®ã€æœç´¢è·³è½¬ç­‰å¹²æ‰°ã€‚
    """
    links = set()
    for a_tag in soup.find_all('a', href=True):
        link = a_tag['href']
        full_url = urljoin(base_url, link)
        
        # é’ˆå¯¹ Kommersant çš„è¿‡æ»¤è§„åˆ™ï¼š
        # 1. é“¾æ¥ä¸­å¿…é¡»åŒ…å« '/doc/'ï¼ˆæ–‡ç« æ ‡è¯†ï¼‰
        # 2. æ’é™¤æ‰åŒ…å« 'page=' æˆ– 'search_query' çš„åˆ†é¡µ/é‡å¤æœç´¢é“¾æ¥
        if '/doc/' in full_url:
            if 'page=' not in full_url and 'search_query' not in link:
                # è§„èŒƒåŒ–ï¼šç§»é™¤ URL æœ«å°¾å¯èƒ½å­˜åœ¨çš„å‚æ•°ï¼Œé˜²æ­¢é‡å¤
                clean_url = full_url.split('?')[0]
                links.add(clean_url)
    return links

# --- æ”¹è¿›ç‰ˆï¼šè‡ªåŠ¨æ£€æµ‹ç»“æŸç‚¹ ---
def extract_article_links(url, limit=None):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Referer': 'https://www.google.com/'
        }
        
        all_links = set()
        current_page = 1
        no_new_content_count = 0  # è®¡æ•°å™¨ï¼šè¿ç»­å¤šå°‘é¡µæ²¡å‘ç°æ–°æ–‡ç« 

        print("ğŸ” å¼€å§‹è‡ªåŠ¨æ¢æµ‹åˆ†é¡µæŠ“å–...")

        while True:
            # æ„é€ å¸¦é¡µç çš„æœç´¢ URL
            page_url = _set_query_param(url, 'page', current_page)
            print(f"æ­£åœ¨å°è¯•ç¬¬ {current_page} é¡µ: {page_url}")
            
            try:
                # è®¾ç½®è¶…æ—¶ï¼Œé˜²æ­¢æ­»æŒ‚
                response = requests.get(page_url, headers=headers, timeout=15)
                if response.status_code != 200:
                    print(f"ğŸ›‘ åœæ­¢ï¼šæœåŠ¡å™¨è¿”å›çŠ¶æ€ç  {response.status_code}")
                    break
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æå–è¿™ä¸€é¡µä¸­ç¬¦åˆè§„åˆ™çš„æ–‡ç« é“¾æ¥
                new_links = _collect_links_from_html(soup, page_url)
                
                before_count = len(all_links)
                all_links.update(new_links)
                after_count = len(all_links)
                
                new_added = after_count - before_count
                
                if new_added > 0:
                    print(f"  âœ… å‘ç° {new_added} ä¸ªæ–°æ–‡ç« é“¾æ¥ï¼Œç´¯è®¡ {after_count}")
                    no_new_content_count = 0  # åªè¦æœ‰æ–°å†…å®¹ï¼Œé‡ç½®è®¡æ•°å™¨
                else:
                    no_new_content_count += 1
                    print(f"  âš ï¸ æœ¬é¡µæœªå‘ç°æ–°æ–‡ç« å†…å®¹ (ç©ºç»“æœæˆ–å†…å®¹é‡å¤ï¼Œç´¯è®¡æ¬¡æ•°: {no_new_content_count})")

                # ã€è‡ªåŠ¨åœæ­¢é€»è¾‘ã€‘
                # å¦‚æœè¿ç»­ 2 é¡µéƒ½æ²¡æœ‰æŠ“åˆ°ä»»ä½•â€œæ–°â€çš„æ–‡ç« é“¾æ¥ï¼Œè¯´æ˜å·²ç»å½»åº•è·‘å‡ºäº†æœç´¢ç»“æœèŒƒå›´
                if no_new_content_count >= 2:
                    print("\nğŸ æ¢æµ‹ç»“æŸï¼šåç»­é¡µé¢å·²æ— æ–°å†…å®¹ï¼Œç¨‹åºè‡ªåŠ¨åœæ­¢ã€‚")
                    break

                # æ€»é‡é™åˆ¶ï¼ˆå¦‚æœä½ åœ¨ main é‡Œè®¾ç½®äº† limit å‚æ•°ï¼‰
                if limit and after_count >= limit:
                    print(f"ğŸš© å·²è¾¾åˆ°è®¾å®šçš„æ€»é‡é™åˆ¶: {limit}")
                    break

                current_page += 1
                # éšæœºä¼‘çœ  1-2 ç§’ï¼Œé˜²æ­¢è§¦å‘åçˆ¬
                time.sleep(random.uniform(1.0, 2.2))

            except Exception as e:
                print(f"âŒ è®¿é—®ç¬¬ {current_page} é¡µå‡ºé”™: {e}")
                break

        return list(all_links)[:limit] if limit else list(all_links)
        
    except Exception as e:
        print(f"æå–é“¾æ¥é”™è¯¯: {e}")
        return []

# --- æå–æ­£æ–‡è¯­æ–™ ---
def extract_sentences_with_keyword(url, keyword):
    try:
        headers = {'User-Agent': random.choice([
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/121.0.0.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Safari/537.36'
        ])}
        
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        
        if response.status_code != 200:
            return []
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ç§»é™¤å¹²æ‰°æ ‡ç­¾
        for s in soup(["script", "style", "nav", "footer"]):
            s.decompose()
        
        text = soup.get_text()
        # é€‚é…ä¸­ä¿„è‹±å¸¸ç”¨åˆ†å¥ç¬¦å·
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\.\!\?ï¼›;]+', text)
        
        matching_sentences = []
        for s in sentences:
            clean_s = s.strip()
            if clean_s and keyword.lower() in clean_s.lower():
                # è¿‡æ»¤å¤ªçŸ­çš„å™ªéŸ³ï¼ˆå¦‚èœå•è¯ï¼‰
                if len(clean_s) > 10:
                    matching_sentences.append(clean_s)
        
        return matching_sentences
        
    except Exception:
        return []

# --- ä¿å­˜ ---
def save_results_to_csv(all_results, keyword, output_file):
    with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(['åºå·', 'æ¥æºURL', 'è¯­å¥å†…å®¹', 'å…³é”®è¯'])
        for idx, (url, sentence) in enumerate(all_results, 1):
            writer.writerow([idx, url, sentence, keyword])
    print(f"\nâœ“ æˆåŠŸï¼è¯­æ–™å·²ä¿å­˜è‡³: {output_file}")

# --- æ‰§è¡Œ ---
def main():
    # è¿™é‡Œä¸éœ€è¦æ”¹ page å‚æ•°ï¼Œç¨‹åºä¼šè‡ªåŠ¨å¾ªç¯
    base_search_url = "https://www.kommersant.ru/search/results?places=&categories=&datestart=2025-02-01&dateend=2026-02-01&sort_type=0&regions=&results_count=&search_query=Huawei"
    keyword = "Huawei"
    
    print("=" * 60)
    print(f"ğŸš€ å¯åŠ¨è‡ªåŠ¨åˆ†é¡µçˆ¬è™« | å…³é”®è¯: {keyword}")
    print("=" * 60)
    
    # ç¬¬ä¸€æ­¥: è‡ªåŠ¨æå–æ‰€æœ‰æœ‰æ•ˆé“¾æ¥
    article_links = extract_article_links(base_search_url)
    
    if not article_links:
        print("âŒ æœªæŠ“å–åˆ°ä»»ä½•æœ‰æ•ˆé“¾æ¥ã€‚")
        return
    
    print(f"\nğŸ”— å…±è®¡è·å– {len(article_links)} ä¸ªæ–‡ç« é“¾æ¥ï¼Œå¼€å§‹æå–è¯­æ–™...\n")
    
    # ç¬¬äºŒæ­¥: æå–å…³é”®è¯è¯­å¥
    all_results = []
    for i, link in enumerate(article_links, 1):
        print(f"[{i}/{len(article_links)}] æå–ä¸­: {link[:50]}...")
        sentences = extract_sentences_with_keyword(link, keyword)
        for s in sentences:
            all_results.append((link, s))
        time.sleep(random.uniform(0.5, 1.2)) # ç¤¼è²Œé—´æ­‡
    
    # ç¬¬ä¸‰æ­¥: ä¿å­˜
    if all_results:
        output_file = f"result_{keyword}_{int(time.time())}.csv"
        save_results_to_csv(all_results, keyword, output_file)
    else:
        print("ğŸ“­ æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯çš„è¯­æ–™ã€‚")

if __name__ == "__main__":
    main()