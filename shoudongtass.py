import requests
import csv
import re
import time
import random
from bs4 import BeautifulSoup

# --- é…ç½® ---
INPUT_FILE = "urls.txt"      # ä½ åˆšæ‰ä¿å­˜é“¾æ¥çš„æ–‡ä»¶
OUTPUT_FILE = "huawei_corpus.csv"
KEYWORD = "Huawei"

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'
}

def extract_sentences(url):
    """è®¿é—®æ–‡ç« é“¾æ¥å¹¶æå–åŒ…å«å…³é”®è¯çš„å¥å­"""
    try:
        # å¢åŠ éšæœºå»¶è¿Ÿï¼Œé˜²æ­¢ TASS å°é”ä½ çš„ IP
        time.sleep(random.uniform(0.3, 0.8))
        
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # å®šä½æ­£æ–‡ï¼šTASS å¸¸è§çš„æ­£æ–‡å®¹å™¨
        article = soup.select_one('.article__text, .text-block, .news-text')
        text = article.get_text(" ", strip=True) if article else soup.get_text(" ", strip=True)
        
        # ä¿„è¯­/è‹±è¯­åˆ†å¥
        sentences = re.split(r'(?<=[.!?])\s+', text)
        matches = [s.strip() for s in sentences if KEYWORD.lower() in s.lower() and len(s.strip()) > 10]
        return matches
    except Exception as e:
        print(f"  âŒ æ— æ³•è¯»å– {url}: {e}")
        return []

def main():
    print(f"ğŸš€ å¼€å§‹å¤„ç†æœ¬åœ°é“¾æ¥åˆ—è¡¨...")
    
    # å‡†å¤‡å†™å…¥ CSV
    with open(OUTPUT_FILE, 'w', encoding='utf-8-sig', newline='') as f_out:
        writer = csv.writer(f_out)
        writer.writerow(['åºå·', 'é“¾æ¥', 'æ ‡é¢˜', 'åŒ¹é…è¯­æ–™'])
        
        # è¯»å–ä½ ä¿å­˜çš„é“¾æ¥æ–‡ä»¶
        try:
            with open(INPUT_FILE, 'r', encoding='utf-8') as f_in:
                lines = f_in.readlines()
        except FileNotFoundError:
            print(f"ğŸ›‘ æ‰¾ä¸åˆ° {INPUT_FILE}ï¼Œè¯·å…ˆæ‰§è¡Œç¬¬ä¸€æ­¥æå–é“¾æ¥ã€‚")
            return

        count = 1
        for i, line in enumerate(lines):
            if ',' not in line: continue
            
            title, url = line.strip().split(',', 1)
            print(f"[{i+1}/{len(lines)}] æ­£åœ¨æå–: {title[:20]}...")
            
            sentences = extract_sentences(url)
            for s in sentences:
                writer.writerow([count, url, title, s])
                count += 1
            
            # æ¯ 10 ç¯‡ä¿å­˜ä¸€æ¬¡ï¼Œé˜²æ­¢ç¨‹åºå´©æºƒä¸¢å¤±æ•°æ®
            if (i + 1) % 10 == 0:
                f_out.flush()
                print(f"ğŸ’¾ å·²ä¿å­˜å‰ {i+1} ç¯‡çš„ç»“æœ")

    print(f"âœ¨ ä»»åŠ¡å®Œæˆï¼è¯­æ–™å·²å­˜å…¥ {OUTPUT_FILE}")

if __name__ == "__main__":
    main()