#!/usr/bin/env python
# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
import csv
import re
from urllib.parse import urljoin, urlsplit, urlunsplit, parse_qsl, urlencode
import time
import random

# --- è¾…åŠ©å‡½æ•°ï¼šå¤„ç† URL å‚æ•° ---
def _set_query_param(url, key, value):
    parts = urlsplit(url)
    query = dict(parse_qsl(parts.query, keep_blank_values=True))
    query[str(key)] = str(value)
    new_query = urlencode(query, doseq=True)
    return urlunsplit((parts.scheme, parts.netloc, parts.path, new_query, parts.fragment))

# --- æ ¸å¿ƒæå–é€»è¾‘ï¼šå¢åŠ æ–‡ç« ç‰¹å¾è¿‡æ»¤ ---
def _collect_links_from_html(soup, base_url):
    links = set()
    for a_tag in soup.find_all('a', href=True):
        link = a_tag['href']
        full_url = urljoin(base_url, link)
        if '/doc/' in full_url:
            if 'page=' not in full_url and 'search_query' not in link:
                clean_url = full_url.split('?')[0]
                links.add(clean_url)
    return links

# --- æ”¹è¿›ç‰ˆï¼šå…·å¤‡â€œåæ‹¦æˆªè‡ªæ„ˆâ€çš„åˆ†é¡µæå– ---
def extract_article_links(url, limit=None):
    try:
        all_links = set()
        current_page = 1
        no_new_content_count = 0  
        
        # æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨å¤´éƒ¨
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Referer': 'https://www.google.com/'
        }

        print("ğŸ” å¼€å§‹è‡ªé€‚åº”åˆ†é¡µæŠ“å–ï¼ˆæ”¯æŒé˜²å°é‡è¯•ï¼‰...")

        while True:
            page_url = _set_query_param(url, 'page', current_page)
            print(f"æ­£åœ¨å°è¯•ç¬¬ {current_page} é¡µ: {page_url}")
            
            try:
                response = requests.get(page_url, headers=headers, timeout=20)
                
                # --- æ ¸å¿ƒæ”¹è¿›ï¼šäººæœºæ ¡éªŒ/é¢‘ç‡é™åˆ¶è¯†åˆ« ---
                if response.status_code in [403, 429] or "captcha" in response.text.lower():
                    wait_time = random.uniform(80, 150) # è§¦å‘å°é”åæ·±åº¦ä¼‘çœ 
                    print(f"\nâš ï¸ æ£€æµ‹åˆ°äººæœºéªŒè¯æˆ–è®¿é—®å—é™ (Code: {response.status_code})")
                    print(f"ğŸ›‘ ç¨‹åºå°†ä¼‘çœ  {int(wait_time)} ç§’ä»¥è§£é™¤å°é”ï¼Œéšåé‡è¯•å½“å‰é¡µ...")
                    time.sleep(wait_time)
                    continue  # è·³è¿‡æœ¬æ¬¡å¾ªç¯ï¼Œé‡æ–°è¯·æ±‚å½“å‰ current_page

                if response.status_code != 200:
                    print(f"âŒ å¼‚å¸¸çŠ¶æ€ç  {response.status_code}ï¼Œ5ç§’åå°è¯•ä¸‹ä¸€é¡µ...")
                    time.sleep(5)
                    current_page += 1
                    continue

                # --- æ­£å¸¸è§£ææµç¨‹ ---
                soup = BeautifulSoup(response.content, 'html.parser')
                new_links = _collect_links_from_html(soup, page_url)
                
                before_count = len(all_links)
                all_links.update(new_links)
                after_count = len(all_links)
                
                new_added = after_count - before_count
                
                if new_added > 0:
                    print(f"  âœ… å‘ç° {new_added} ä¸ªæ–°æ–‡ç« é“¾æ¥ï¼Œç´¯è®¡ {after_count}")
                    no_new_content_count = 0 
                else:
                    # åªæœ‰åœ¨è¯·æ±‚æˆåŠŸä½†æ²¡å†…å®¹æ—¶ï¼Œæ‰è®¤ä¸ºå¯èƒ½åˆ°åº•äº†
                    no_new_content_count += 1
                    print(f"  âš ï¸ æœ¬é¡µæœªå‘ç°æ–°æ–‡ç« å†…å®¹ (ç©ºç»“æœè®¡æ•°: {no_new_content_count})")

                # å¦‚æœè¿ç»­ 3 é¡µæˆåŠŸè¯·æ±‚ä½†éƒ½æ²¡æœ‰æ–°æ–‡ç« ï¼Œæ‰çœŸæ­£åœæ­¢
                if no_new_content_count >= 3:
                    print("\nğŸ æ¢æµ‹ç»“æŸï¼šè¿ç»­å¤šé¡µæ— æ–°å†…å®¹ï¼Œè‡ªåŠ¨åœæ­¢ã€‚")
                    break

                if limit and after_count >= limit:
                    break

                current_page += 1
                # æ­£å¸¸çš„æ­¥è¿›éšæœºä¼‘çœ 
                time.sleep(random.uniform(2.5, 4.5))

            except (requests.exceptions.RequestException, Exception) as e:
                print(f"âŒ ç½‘ç»œæ³¢åŠ¨æˆ–å¼‚å¸¸: {e}ï¼Œæ­£åœ¨é‡è¯•å½“å‰é¡µ...")
                time.sleep(10)
                continue

        return list(all_links)[:limit] if limit else list(all_links)
        
    except Exception as e:
        print(f"æå–é“¾æ¥å¼‚å¸¸: {e}")
        return []

# --- æå–è¯­æ–™å‡½æ•° ---
def extract_sentences_with_keyword(url, keyword):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/121.0.0.0'}
        response = requests.get(url, headers=headers, timeout=15)
        
        # è¯­æ–™æå–é˜¶æ®µå¦‚æœé‡åˆ°æ‹¦æˆªï¼ŒåŒæ ·å¢åŠ ä¿æŠ¤
        if response.status_code in [403, 429]:
            print(f"\nâš ï¸ è¯¦æƒ…é¡µè®¿é—®å—é™ï¼Œä¼‘çœ  30 ç§’...")
            time.sleep(30)
            return []

        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.content, 'html.parser')
        for s in soup(["script", "style", "nav", "footer"]):
            s.decompose()
        
        text = soup.get_text()
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\.\!\?ï¼›;]+', text)
        
        matching_sentences = []
        for s in sentences:
            clean_s = s.strip()
            if clean_s and keyword.lower() in clean_s.lower():
                if len(clean_s) > 10:
                    matching_sentences.append(clean_s)
        return matching_sentences
    except:
        return []

# --- ä¿å­˜ç»“æœ ---
def save_results_to_csv(all_results, keyword, output_file):
    with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(['åºå·', 'æ¥æºURL', 'è¯­å¥å†…å®¹', 'å…³é”®è¯'])
        for idx, (url, sentence) in enumerate(all_results, 1):
            writer.writerow([idx, url, sentence, keyword])
    print(f"\nâœ“ æˆåŠŸï¼ä¿å­˜è‡³: {output_file}")

# --- ä¸»ç¨‹åº ---
def main():
    base_search_url = "https://www.kommersant.ru/search/results?search_query=Huawei&sort_type=0&search_full=1&time_range=2&dateStart=2020-01-02&dateEnd=2026-02-02"
    keyword = "Huawei"
    
    print("=" * 60)
    print(f"ğŸš€ å¯åŠ¨è‡ªä¿®å¤åˆ†é¡µçˆ¬è™« | å…³é”®è¯: {keyword}")
    print("=" * 60)

    # 1. æŠ“å–é“¾æ¥
    article_links = extract_article_links(base_search_url)
    
    if not article_links:
        print("âŒ æœªè·å–åˆ°æœ‰æ•ˆé“¾æ¥ã€‚")
        return
    
    print(f"\nğŸ”— å…±è®¡è·å– {len(article_links)} ä¸ªé“¾æ¥ï¼Œå¼€å§‹æå–è¯­æ–™...\n")
    
    # 2. æå–è¯­å¥
    all_results = []
    for i, link in enumerate(article_links, 1):
        print(f"[{i}/{len(article_links)}] æå–ä¸­: {link[:50]}...")
        sentences = extract_sentences_with_keyword(link, keyword)
        for s in sentences:
            all_results.append((link, s))
        
        # è¯¦æƒ…é¡µçˆ¬å–ä¹Ÿå»ºè®®ç¨å¾®æ”¾æ…¢é€Ÿåº¦
        time.sleep(random.uniform(0.8, 1.5))
    
    # 3. ä¿å­˜
    if all_results:
        output_file = f"result_{keyword}_{int(time.time())}.csv"
        save_results_to_csv(all_results, keyword, output_file)
    else:
        print("ğŸ“­ æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯çš„è¯­æ–™ã€‚")

if __name__ == "__main__":
    main()